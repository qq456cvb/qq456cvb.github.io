2025.08: We're hosting the cross-disciplinary workshop ``WCLOP: 1st ICCV Workshop and Challenge on Category-Level Object Pose Estimation in the Wild'' at ICCV 2025, spotlighting advances in category-level object pose estimation for robotic manipulation scenarios. We are now accepting paper submissions and challenge participants! Awards totaling over $6000 are available! Please see our <a href='https://wclop.github.io/'>website</a> for more details.

2025.08: Two papers got accepted to CoRL 2025! Details coming soon.

2025.06: Our paper: <a href='https://arxiv.org/abs/2506.07310'>AllTracker: Efficient Dense Point Tracking at High Resolution</a> is accepted to <i>ICCV 2025</i>! <a href='https://arxiv.org/pdf/2506.07310'>[Paper]</a> <a href='https://github.com/aharley/alltracker'>[Code]</a> <a href='https://alltracker.github.io/'>[Project Page]</a>

2025.01: Our paper: <a href='https://arxiv.org/abs/2411.19458'>Multiview Equivariance Improves 3D Correspondence Understanding with Minimal Feature Finetuning</a> is accepted to <i>ICLR 2025</i>! <a href='https://arxiv.org/pdf/2411.19458'>[Paper]</a> <a href='https://github.com/qq456cvb/3DCorrEnhance'>[Code]</a> <a href='https://huggingface.co/spaces/qq456cvb/3DCorrEnhance'>[Huggingface Demo]</a><a href='https://qq456cvb.github.io/3DCorrEnhance'>[Project Page]</a>

2025.01: Our paper: <a href='https://arxiv.org/abs/2311.02787'>Make a Donut: Language-Guided Hierarchical EMD-Space Planning for Zero-shot Deformable Object Manipulation</a> is accepted to <i>IEEE Robotics and Automation Letters (RA-L)</i>! <a href='https://arxiv.org/pdf/2311.02787'>[Paper]</a> <a href='#'>[Code (Coming soon)]</a> <a href='/projects/donut'>[Project Page]</a>

2024.10: Our paper: <a href='https://arxiv.org/abs/2401.08140'>ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process</a> is accepted to <i>NeurIPS 2024</i>! <a href='https://arxiv.org/pdf/2401.08140'>[Paper]</a> <a href='https://provnerf.github.io/'>[Project Page]</a>

2024.07: Our paper: <a href='https://arxiv.org/abs/2312.15130'>PACE: Pose Annotations in Cluttered Environments</a> is accepted to <i>ECCV 2024</i>! <a href='https://arxiv.org/pdf/2312.15130'>[Paper]</a> <a href='https://github.com/qq456cvb/PACE'>[Code]</a> <a href='/projects/pace'>[Project Page]</a>

2024.07: Our paper: <a href='https://arxiv.org/abs/2310.04189'>Bridging the Gap Between Human Motion and Action Semantics via Kinematics Phrases</a> is accepted to <i>ECCV 2024</i>! <a href='https://arxiv.org/pdf/2310.04189'>[Paper]</a> <a href='https://foruck.github.io/KP/'>[Project Page]</a>

2024.06: Our paper: <a href='https://arxiv.org/abs/2403.16023'>RPMArt: Towards Robust Perception and Manipulation for Articulated Objects</a> is accepted to <i>IROS 2024</i>! <a href='https://arxiv.org/pdf/2403.16023'>[Paper]</a> <a href='https://github.com/R-PMArt/rpmart'>[Code]</a> <a href='https://r-pmart.github.io/'>[Project Page]</a>

2024.06: Our paper: <a href='https://arxiv.org/abs/2211.13398'>CPPF++: Uncertainty-Aware Sim2Real Object Pose Estimation by Vote Aggregation</a> is accepted to <i>TPAMI 2024</i>! This is perhaps the most generalizable category-level pose estimation method you can find, and it only requires cheap synthetic data for training! <a href='https://arxiv.org/abs/2211.13398'>[Paper]</a> <a href='https://github.com/qq456cvb/CPPF2'>[Code]</a> <a href='/projects/cppf++'>[Project Page]</a>

2024.01: Our paper: <a href='https://arxiv.org/abs/2310.16838'>SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation</a> is accepted to <i>ICLR 2024</i>! <a href='https://arxiv.org/abs/2310.16838'>[Paper]</a> <a href='https://halowangqx.github.io/SparseDFF'>[Code]</a>

2023.12: Our paper: <a href='https://arxiv.org/abs/2312.10714'>Primitive-based 3D Human-Object Interaction Modelling and Programming</a> is accepted to <i>AAAI 2024</i>! <a href='https://arxiv.org/abs/2312.10714'>[Paper]</a> <a href='https://mvig-rhos.com/p3haoi'>[Code]</a>

2023.03: Our paper: <a href='https://arxiv.org/abs/2303.03101'>CRIN: Rotation-Invariant Point Cloud Analysis and Rotation Estimation via Centrifugal Reference Frame</a> is accepted to <i>AAAI 2023</i> <b>Oral</b>! <a href='hhttps://arxiv.org/abs/2303.03101'>[Paper]</a> <a href='https://github.com/yokinglou/CRIN'>[Code]</a>

2022.03: Our paper: <a href='https://arxiv.org/abs/2011.11974'>UKPGAN: A General Self-Supervised Keypoint Detector</a> is accepted to <i>CVPR 2022</i>! <a href='https://arxiv.org/abs/2011.11974'>[Paper]</a> <a href='https://github.com/qq456cvb/UKPGAN'>[Code]</a> <a href='/projects/ukpgan'>[Project Page]</a>

2022.03: Our paper: <a href='https://arxiv.org/abs/2011.12001'>Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes</a> is accepted to <i>CVPR 2022</i>! <a href='https://arxiv.org/abs/2011.12001'>[Paper]</a> <a href='https://github.com/qq456cvb/CanonicalVoting'>[Code]</a> <a href='/projects/canonical-voting'>[Project Page]</a>

2022.03: Our paper: <a href='https://arxiv.org/abs/2203.03089'>CPPF: Towards Robust Category-Level 9D Pose Estimation in the Wild</a> is accepted to <i>CVPR 2022</i>! <a href='https://arxiv.org/abs/2203.03089'>[Paper]</a> <a href='https://github.com/qq456cvb/CPPF'>[Code]</a> <a href='/projects/cppf'>[Project Page]</a>

2021.11: Our paper: <a href='https://arxiv.org/abs/2102.12093'>PRIN/SPRIN: On Extracting Point-wise Rotation Invariant Features</a> is accepted to <i>TPAMI</i>, and to appear in the upcoming issues! <a href='https://arxiv.org/abs/2102.12093'>[Paper]</a> <a href='https://github.com/qq456cvb/SPRIN'>[Code]</a> <a href='/sprin'>[Project Page]</a>

2021.04: Our paper: <a href='https://arxiv.org/abs/2111.10817'>Understanding Pixel-level 2D Image Semantics with 3D Keypoint Knowledge Engine</a> is accepted to <i>TPAMI</i>, and to appear in the upcoming issues! <a href='https://arxiv.org/abs/2111.10817'>[Paper]</a> <a href='/pixel-understanding'>[Project Page]</a>

2021.02: Our paper <a href='https://arxiv.org/abs/2103.10814.pdf'>Skeleton Merger: an Unsupervised Aligned Keypoint Detector</a> is accepted as <i>CVPR</i> 2021 <b>Oral</b>! <a href='https://arxiv.org/abs/2103.10814'>[Paper]</a> <a href='https://github.com/eliphatfs/SkeletonMerger'>[Code]</a>

<!-- 2020.06: Our code and full dataset for <a href='/keypointnet'>KeypointNet</a> are released on <a href='https://github.com/qq456cvb/KeypointNet'>Github</a>! -->

2020.02: Our paper <a href='https://arxiv.org/abs/2002.12687'>KeypointNet: A Large-scale 3D Keypoint Dataset Aggregated from Numerous Human Annotations</a> is accepted to <i>CVPR</i> 2020! <a href='https://arxiv.org/abs/2002.12687'>[Paper]</a> <a href='https://github.com/qq456cvb/KeypointNet'>[Code]</a> <a href='/keypointnet'>[Project Page]</a>